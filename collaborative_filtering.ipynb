{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews and items are from the output of the `preprocessing.ipynb` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('../datasets/slimmed/reviews.csv')\n",
    "items = pd.read_csv('../datasets/slimmed/items.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to get title of item from its id (parent_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_name_from_id(parent_asin):\n",
    "\treturn items[items['parent_asin'] == parent_asin]['title'].unique()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the user-item matrix would be too large to fit in memory and would contain many zero values anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_ids, num_item_ids = reviews['user_id'].nunique(), items['parent_asin'].nunique()\n",
    "format(num_user_ids, ','), format(num_item_ids, ','), format(num_user_ids * num_item_ids, ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix and mappings from `user_item_matrix` are imported into `uim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load the notebook\n",
    "with open('user_item_matrix.ipynb', 'r', encoding='utf-8') as f:\n",
    "\tnb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Execute all code cells and store data in the uim dict\n",
    "uim = {}\n",
    "for cell in nb.cells:\n",
    "\tif cell.cell_type == 'code':\n",
    "\t\texec(cell.source, uim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Model (Alternating Least Squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`implicit` library already uses multithreading so `BLAS` threads should be set to 1 to avoid overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threadpoolctl \n",
    "threadpoolctl.threadpool_limits(1, 'blas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming CSR Ratings To Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core issue here is that implicit's ALS model works with implicit feedback and not explicit ones such as rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract components\n",
    "data = uim['sparse_matrix_csr'].data\n",
    "indices = uim['sparse_matrix_csr'].indices\n",
    "indptr = uim['sparse_matrix_csr'].indptr\n",
    "\n",
    "# Compute per-user mean ratings\n",
    "n_users = uim['sparse_matrix_csr'].shape[0]\n",
    "\n",
    "user_means = np.zeros(n_users)\n",
    "max_user_ratings = np.zeros(n_users)\n",
    "\n",
    "for user in range(n_users):\n",
    "\tstart, end = indptr[user], indptr[user + 1]\n",
    "\tuser_ratings = data[start:end]\n",
    "\n",
    "\tif len(user_ratings) > 0:\n",
    "\t\tuser_means[user] = np.mean(user_ratings)\n",
    "\t\tmax_user_ratings[user] = np.max(user_ratings)\n",
    "\telse:\n",
    "\t\tuser_means[user] = 0.0\n",
    "\t\tmax_user_ratings[user] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ALPHA` is scaling factor that determines how strongly the higher ratings are trusted over low ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following strategy is proposed for handling this<br><br>\n",
    "For every item with mean user rating $\\mu_u$<br>\n",
    "o If an item rating is less than $\\mu_u$, then it is set to 0 (considered as not seen)<br>\n",
    "o Otherwise, it is scaled to a value in the range [1, 5] using min-max normalization of min=$\\mu_u$ and max=max_user_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.copy()\n",
    "\n",
    "for user in range(n_users):\n",
    "\tstart, end = indptr[user], indptr[user + 1]\n",
    "\tfor i in range(start, end):\n",
    "\t\trating = data[i]\n",
    "\t\tmean = user_means[user]\n",
    "\t\tmax_rating = max_user_ratings[user]\n",
    "\n",
    "\t\tif rating < mean:\n",
    "\t\t\tnew_data[i] = 0  # no confidence\n",
    "\t\telse:\n",
    "\t\t\t# # If user only gave ratings of 5, then it can be considered as the \"neutral\" rating\n",
    "\t\t\t# if mean == 5:\n",
    "\t\t\t#     conf = 3\n",
    "\t\t\t# # Linear map from [mean, 5] to [1, 5]\n",
    "\t\t\t# else:\n",
    "\t\t\t#     conf = (rating - mean) / (5 - mean) * 4 + 1\n",
    "\n",
    "\t\t\ts = 0\n",
    "\t\t\tif max_rating == mean:\n",
    "\t\t\t\ts = 1.0\n",
    "\t\t\telse:\n",
    "\t\t\t\ts = (rating - mean) / (max_rating - mean)\n",
    "\n",
    "\t\t\tnew_data[i] = 1 + ALPHA * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "confidence_csr = csr_matrix((new_data, indices, indptr), shape=uim['sparse_matrix_csr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_csr.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing k Latent Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be best to evaluate the model against the ratings of users with the most number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns indices of top n users who've reviewed the most items\n",
    "def getFrequentReviewersIdx(n):\n",
    "    userReviewTotals = reviews.groupby(\"user_id\").size().reset_index(name=\"total_reviews\")\n",
    "    mostFreqReviewers = userReviewTotals.sort_values(by=\"total_reviews\", ascending=False)[:n]\n",
    "    return mostFreqReviewers[\"user_id\"].map(uim[\"user_map\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the indices and ratings of all items a user has reviewed\n",
    "def getRatings(user_idx):\n",
    "    ratings = uim[\"sparse_matrix_csr\"][user_idx, :].toarray().flatten()\n",
    "    itemIndices = ratings.nonzero()[0]\n",
    "    ratings = ratings[itemIndices]\n",
    "\n",
    "    return list(zip(itemIndices, ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_to_predicted_rating(user_id, confidences):\n",
    "    mean = user_means[user_id]\n",
    "    max_rating = max_user_ratings[user_id]\n",
    "\n",
    "    s = (confidences - 1) / ALPHA\n",
    "    return mean + s * (max_rating - mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation metric for optimizing k will be RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculates RMSE of a model on ratings of the most active reviewers\n",
    "def evalRMSE(als_model, topReviewers):\n",
    "    user_factors = als_model.user_factors\n",
    "    item_factors = als_model.item_factors\n",
    "\n",
    "    # Cumulative arrays containing all users' ratings and predictions\n",
    "    allRatings = []\n",
    "    allPredictions = []\n",
    "    \n",
    "    for user_index in topReviewers:\n",
    "        ratedItems, ratings = zip(*getRatings(user_index))\n",
    "        ratings = list(ratings)\n",
    "        # Implicit ALS model doesn't have .predict(), so we use dot prod @ between user_factors and item_factors to predict specific ratings (without bias)\n",
    "        predictedRatingsConf = np.array([user_factors[user_index] @ item_factors[item_index] for item_index in ratedItems])\n",
    "        # Above calculates confidence (implicit), so we need to convert to rating (explicit)\n",
    "        predictedRatings = [confidence_to_predicted_rating(user_index, confidence) for confidence in predictedRatingsConf]\n",
    "        \n",
    "        allRatings.extend(ratings)\n",
    "        allPredictions.extend(predictedRatings)\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(allRatings, allPredictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(\"RMSE\")\n",
    "    print(rmse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# Finds best k latent features given confidence matrix and number of reviewers we want to use to evaluate RMSE\n",
    "def optimizeK(kVals, confidence_csr, num_reviewers):\n",
    "\n",
    "    topReviewers = getFrequentReviewersIdx(num_reviewers)  \n",
    "    bestK = None\n",
    "    bestRMSE = float(\"inf\")\n",
    "\n",
    "    for k in kVals:\n",
    "        \n",
    "        als_model = AlternatingLeastSquares(factors=k, iterations=15, regularization=0.1, random_state=42, calculate_training_loss=True)\n",
    "        als_model.fit(confidence_csr)\n",
    "\n",
    "        rmse = evalRMSE(als_model, topReviewers)\n",
    "\n",
    "        if rmse < bestRMSE:\n",
    "            bestRMSE = rmse\n",
    "            bestK = k\n",
    "            \n",
    "\n",
    "    return bestK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kVals = [5, 10, 15, 20, 25]\n",
    "bestK = optimizeK(kVals, confidence_csr, 15)\n",
    "print(f\"The best k value is {bestK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ALS model is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ALS model\n",
    "als_model = AlternatingLeastSquares(factors=bestK, iterations=15, regularization=0.1, random_state=42, calculate_training_loss=True)\n",
    "als_model.fit(confidence_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Save to a pickle file\n",
    "with gzip.open('../data_structures/als_model.pkl', 'wb', compresslevel=5) as f:\n",
    "\tpickle.dump(als_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from typing import cast\n",
    "from implicit.cpu.als import AlternatingLeastSquares\n",
    "\n",
    "# Load the compressed file\n",
    "with gzip.open('../data_structures/als_model.pkl', 'rb') as f:\n",
    "\tals_model = cast(AlternatingLeastSquares, pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionRecallK(model, test_users, k):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for user_index in test_users:\n",
    "        # Ground truth relevant items and indices\n",
    "        relevantItems = (getRatings(user_index))\n",
    "        relevantIndices = set([item for item, _ in relevantItems])\n",
    "\n",
    "        # k recommended items and indices\n",
    "        recommendedItems = model.recommend(user_index, uim['sparse_matrix_csr'][user_index], N=k, filter_already_liked_items=False)\n",
    "        recommendations, scores = recommendedItems\n",
    "        recommendations_scores = zip(recommendations, scores)\n",
    "        recommendationIndices = set([item_id for item_id, score in recommendations_scores])\n",
    "\n",
    "        # Relevant items in top k\n",
    "        overlap = recommendationIndices & relevantIndices\n",
    "\n",
    "        precision = len(overlap) / k\n",
    "        recall = len(overlap) / len(relevantIndices)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)          \n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotPRK(model, test_users, ks):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for k in ks:\n",
    "        p, r = precisionRecallK(model, test_users, k)\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ks, precisions, label='Precision@k', marker='o')\n",
    "    plt.plot(ks, recalls, label='Recall@k', marker='x')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Precision and Recall vs k Recommendations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUsers = getFrequentReviewersIdx(10)\n",
    "plotPRK(als_model, testUsers, kVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotF1K(model, test_users, ks):\n",
    "    f1s = []\n",
    "\n",
    "    for k in ks:\n",
    "        precision, recall = precisionRecallK(model, test_users, k)\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ks, f1s, label='F1 Score@k', marker='s')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs k Recommendations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotF1K(als_model, testUsers, kVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKRMSE(k_values, rmse_values):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(k_values, rmse_values, marker='o')\n",
    "    plt.xlabel('k Latent Factors ')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('RMSE vs. k Latent Features')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelK5 = AlternatingLeastSquares(factors=10, iterations=15, regularization=0.1, random_state=42, calculate_training_loss=True)\n",
    "modelK5.fit(confidence_csr)\n",
    "\n",
    "modelK10 = AlternatingLeastSquares(factors=10, iterations=15, regularization=0.1, random_state=42, calculate_training_loss=True)\n",
    "modelK10.fit(confidence_csr)\n",
    "\n",
    "modelK15 = AlternatingLeastSquares(factors=15, iterations=15, regularization=0.1, random_state=42, calculate_training_loss=True)\n",
    "modelK15.fit(confidence_csr)\n",
    "\n",
    "modelK20 = AlternatingLeastSquares(factors=20, iterations=15, regularization=0.1, random_state=42, calculate_training_loss=True)\n",
    "modelK20.fit(confidence_csr)\n",
    "\n",
    "modelK25 = AlternatingLeastSquares(factors=25, iterations=15, regularization=0.1, random_state=42, calculate_training_loss=True)\n",
    "modelK25.fit(confidence_csr)\n",
    "\n",
    "\n",
    "models = [modelK5, modelK10, modelK15, modelK20, modelK25]\n",
    "\n",
    "rmseVals = [evalRMSE(model, testUsers) for model in models]\n",
    "\n",
    "plotKRMSE(kVals, rmseVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting User Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test run where the top 5 items are recommended for user with id from the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 2  # Target user\n",
    "num_recommendations = 15  # How many items to recommend\n",
    "\n",
    "# Get top N recommended items and their scores\n",
    "recommended_items = als_model.recommend(\n",
    "\tuser_id, uim['sparse_matrix_csr'][user_id], N=num_recommendations\n",
    ")\n",
    "\n",
    "recommendations, scores = recommended_items\n",
    "recommendations_scores = zip(recommendations, scores)\n",
    "\n",
    "print(f'Top {num_recommendations} recommended items for User {uim['reverse_user_map'][user_id]}:')\n",
    "for item_id, score in recommendations_scores:\n",
    "\tprint(f'Item {uim['reverse_item_map'][item_id]} - Score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_rated_user_items = reviews[reviews['user_id'] == uim['reverse_user_map'][user_id]][['title', 'parent_asin', 'text', 'rating']]\n",
    "already_rated_user_items[['parent_asin', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items['parent_asin'].isin(already_rated_user_items['parent_asin'])][['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model.recommend(\n",
    "\tuser_id, uim['sparse_matrix_csr'][user_id], items=[3, 4], filter_already_liked_items=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those confidence scores in the items are now converted back to user ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, uim['reverse_user_map'][user_id], user_means[user_id], max_user_ratings[user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to convert confidence scores to predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_to_predicted_rating(user_id, confidences):\n",
    "    mean = user_means[user_id]\n",
    "    max_rating = max_user_ratings[user_id]\n",
    "\n",
    "    s = (confidences - 1) / ALPHA\n",
    "    return mean + s * (max_rating - mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly predicted the user's ratings on items they'd seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_to_predicted_rating(user_id, als_model.recommend(\n",
    "\tuser_id, uim['sparse_matrix_csr'][user_id], items=[3, 4], filter_already_liked_items=False\n",
    ")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted ratings that the user would give to the recommended items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(recommendations, confidence_to_predicted_rating(user_id, recommendations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the recommended items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda i: get_item_name_from_id(uim['reverse_item_map'][i]), recommendations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_similar = 10  # How many similar items to find\n",
    "top_similar_users = als_model.similar_users(user_id, N=num_similar+1)\n",
    "\n",
    "similar_users, scores = top_similar_users\n",
    "similar_users_scores = list(zip(similar_users[1:], scores[1:]))\n",
    "\n",
    "print(f'Top {num_similar} users similar to User {uim['reverse_user_map'][user_id]}:')\n",
    "for sim_user_id, similarity in similar_users_scores[1:]:\n",
    "\tprint(f'User {uim['reverse_user_map'][sim_user_id]} - Similarity Score: {similarity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Similar Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 1  # Target item\n",
    "num_similar = 10  # How many similar items to find\n",
    "\n",
    "# Get top N similar items and their similarity scores (+1 is added to skip the item itself later on)\n",
    "top_similar_items = als_model.similar_items(item_id, N=num_similar+1)\n",
    "\n",
    "similar_items, scores = top_similar_items\n",
    "similar_items_scores = list(zip(similar_items, scores))\n",
    "\n",
    "print(f'Top {num_similar} items similar to Item {uim['reverse_item_map'][item_id]}:')\n",
    "for sim_item_id, similarity in similar_items_scores[1:]:\n",
    "\tprint(f'Item {uim['reverse_item_map'][sim_item_id]} - Similarity Score: {similarity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good and relevant recommendations for the given item (first in the list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items['parent_asin'] == uim['reverse_item_map'][item_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(get_item_name_from_id, map(lambda x: uim['reverse_item_map'][x], [item_id, *similar_items[1:]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Guests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guest (vectors) are not in the ALS matrix and so cannot use the `similar_users` & `recommend_items` above directly but this can be handled<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_vector = ['B07KRWJCQW', 'B07ZJ6RY1W', 'B07JGVX9D6', 'B075YBBQMM', 'B0BN942894', 'B077GG9D5D', 'B00ZQB28XK', 'B014R4KYMS', 'B07YBXFF5C']\n",
    "mapped_guest_vector = uim['item_map'][uim['item_map']['parent_asin'].isin(guest_vector)].index.tolist()\n",
    "\n",
    "mapped_guest_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[get_item_name_from_id(parent_asin) for parent_asin in guest_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`similar_items` only needs item ids (similar_items also includes the given item so N+1 similar items must be generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalized_items = als_model.similar_items(mapped_guest_vector, N=10+1)\n",
    "\n",
    "recommend_items, scores = personalized_items\n",
    "similar_items = list(zip(recommend_items, scores))\n",
    "\n",
    "similar_items[0] # An example print of similar items for 'B07KRWJCQW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, parent_asin in enumerate(guest_vector):\n",
    "    print(f'For {get_item_name_from_id(parent_asin)}')\n",
    "    print(f'The similar items are {[get_item_name_from_id(uim['reverse_item_map'][parent_asin]) for parent_asin in similar_items[idx][0][1:10]]}')\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_item_name_from_id(guest_vector[0]), [get_item_name_from_id(uim['reverse_item_map'][parent_asin]) for parent_asin in similar_items[0][0][1:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more powerful recommendation system can be built using the other features in the `items` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[['title', 'parent_asin', 'features', 'description', 'details', 'categories']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use TF-IDF or BERT Embeddings... (Embeddings would be better as descriptions may not contain similar words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model (OOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is to be used in the backend but this is not possible without all its dependencies being saved as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
